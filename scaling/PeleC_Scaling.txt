Weak scaling in PeleC was studied using Cori, the newest machine at NERSC, which has a large partition of Intel Haswell CPU nodes which have 2 CPU sockets, each with 16 cores, and a much larger partition of Intel Xeon Phi (KNL version) CPU nodes which have 1 CPU socket with 68 cores. The PMF problem described earlier was set up with a box grid with equal resolution in the x,y, and z dimensions. This was a "flat" grid with no adaptive mesh refinement (AMR). Starting with a base value of 128^3 grid cells per Haswell CPU socket, this value, which is ~2 million cells, was kept constant per Haswell socket while increasing the physical domain size and the number of cells in the domain to an increasingly larger amount of CPU sockets in the machine. The results for both the Haswell runs and KNL runs are shown in the figure. The Haswell plot was run using flat MPI, meaning no threading was used, but PeleC can also run using a hybrid of MPI ranks with OpenMP threading which reduces inter-process communication and allows for more efficient use of the machine. This hybrid MPI+OpenMP regime was utilized in the KNL runs where 32 MPI ranks were used per KNL socket, with each MPI rank using 4 threads. Using 4 threads per MPI rank has been shown to currently be the most efficient process mapping for PeleC on KNL as well as Haswell. Core specialization was used on KNL where 4 cores are essentially left idle to allow for the operating system tasks to be handled by these cores. Also, using 2 hyperthreads has been shown to be advantageous on KNL, while 4 hyperthreads shows no advantage. For simplicity, 2*(128^3) grid cells were used per KNL socket which is ~4 million grid cells, so that a Haswell node (2 sockets) and KNL node (1 socket) can be considered similar for the purpose of this study. It is clear that when plotting the runtime against the number of CPU sockets, while keeping the number of grid cells calculated per socket constant, PeleC is showing excellent weak scalability. This is most apparent in the KNL cases ranging from 1 socket to 2048 sockets. With 2048 KNL sockets being the equivalent of a ~6.2 petaflop capable machine, PeleC is providing 94% parallel efficiency in this case. Running on the smaller partition of the Cori Haswell nodes only allows one to allocate <1000 nodes per job, so the plot is cut short there but the scaling results are similar.

Output of PeleC typically comes in the form of plot files containing the set of simulation variables that can be output at requested simulation time step intervals, which can be viewed in VisIt, Paraview, Amrvis, or yt. The other type of output from PeleC are checkpoint files which store the exact state of the simulation on disk and allow for the simulation to be restarted from this state. Since the checkpoint files store the entire state of the simulation, they are typically much larger than the plot files. Therefore, when running and analyzing results for large simulations, the I/O performance of the application becomes significant since the simulation data can easily grow to terabytes and petabytes of data per simulation. PeleC having been built on top of AMReX allows for high performance parallel I/O capabilities AMReX provides.

A study involving the I/O performance characteristics on the Cori machine from NERSC is listed in the provided figures. The first figure shows the actual write time of plot files and checkpoint files for simulations using 160^3 to 1024^3 grid cells, ranging from 7.2GB plot files for 160^3 grid cells, to 1.9TB checkpoint files for a 1024^3 grid cell simulation. The same simulation that was used for the weak scaling study was used here as well, but only for a number of nodes from 1 to 256. The second figure shows the rate at which data was able to be written for these same simulation cases using 1 to 256 nodes. As can be seen, the amount of data output for each simulation increases exponentially as the number of simulation grid cells increases, and therefore the time to write the data increases exponentially as well. However, this is somewhat mitigated by the fact that as the simulation size increases, so does the number of compute nodes used for the simulation, which allows for an increasing amount of throughput to the parallel filesystem, all the way up to 744GB/sec for the entire Cori scratch filesystem. Another way this write time can be mitigated is by using the burst buffer available on Cori which is a parallel filesystem using solid state disks (SSDs), where the aggregate performance now becomes 1.7TB/sec. Results for using the burst buffer to write simulation data is listed in both figures as well. The burst buffer allows for ~2.5x faster write times than when writing directly to the scratch filesystem disks. After the data is written to the burst buffer filesystem, it is later staged into the scratch filesystem, but this happens after the simulation runs. It is seen that even at simulations of 1024^3, a checkpoint file size is almost 2TB. When moving to much larger simulation sizes, the amount of data can then become difficult to manage.

In the future, as with many other exascale targeted applications, in-situ visualization and analysis will become a necessary feature. This capability will be in PeleC and AMReX in the future likely using tools such as Catalyst that many other projects based on the visualization toolkit (VTK) are using for in-situ visualization as well. In-situ analysis is already being explored with AMReX as can be seen in https://comp-astrophys-cosmol.springeropen.com/articles/10.1186/s40668-016-0017-2.